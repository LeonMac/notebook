{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多项式回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成60对，具备一定随机性的数据集 Y = sin(x) + rand\n",
    "x = np.array([i*np.pi/180 for i in range(60,300,4)])\n",
    "np.random.seed(10)  #Setting seed for reproducability\n",
    "y = np.sin(x) + np.random.normal(0,0.15,len(x))\n",
    "data = pd.DataFrame(np.column_stack([x,y]),columns=['x','y'])\n",
    "data.head(10)\n",
    "plt.scatter(data['x'],data['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成多项式特征\n",
    "for i in range(2,16):  \n",
    "    colname = 'x_%d'%i    \n",
    "    # x_2是x的平方；x_3是x的立方，以此类推\n",
    "    data[colname] = data['x'] ** i\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用RSS(残差平方和（Residual Sum of Squares))作为损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def linear_regression(data, power, models_to_plot):\n",
    "    #initialize predictors:\n",
    "    predictors=['x']\n",
    "    if power>=2:\n",
    "        predictors.extend(['x_%d'%i for i in range(2,power+1)])\n",
    "        \n",
    "    print(f\"power = {power}--> {predictors}\")\n",
    "    \n",
    "    #Fit the model\n",
    "    linreg = LinearRegression()#normalize=True)\n",
    "    linreg.fit(data[predictors],data['y'])\n",
    "    # 使用fit后的模型预测得到预测值y_pred\n",
    "    y_pred = linreg.predict(data[predictors])\n",
    "    \n",
    "    # 计算预测值y_pred和实际值y之间的RSS 残差平方和（Residual Sum of Squares）\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    # 创建了一个列表 ret，并将 RSS（残差平方和）作为第一个元素添加到列表中。\n",
    "    ret = [rss]\n",
    "    # 将线性回归模型的截距（intercept）添加到 ret 列表中\n",
    "    ret.extend([linreg.intercept_])\n",
    "    # 将线性回归模型的系数（coefficients）添加到 ret 列表中， linreg.coef_ 包含了每个特征的系数值。\n",
    "    ret.extend(linreg.coef_)\n",
    "    \n",
    "    #Check if a plot is to be made for the entered power\n",
    "    if power in models_to_plot:\n",
    "#         print(f\"power = {power}\")\n",
    "        plt.subplot(models_to_plot[power])\n",
    "        plt.tight_layout()\n",
    "        # 使用线plot y_pred \n",
    "        plt.plot(data['x'],y_pred,lw=3)\n",
    "        # 使用点plot y \n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        \n",
    "        plt.title('Plot for power: %d , RSS: %.2f'% (power,rss))\n",
    "    else:\n",
    "        print(f\"power = {power} donot plot\")\n",
    "        pass\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 18,9\n",
    "\n",
    "#Initialize a dataframe to store the results:\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['model_pow_%d'%i for i in range(1,16)]\n",
    "coef_matrix_simple = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "#Define the power value and plot position for which a plot is required in a dict\n",
    "models_to_plot = {1:231,3:232,6:233,9:234,12:235,15:236}\n",
    "\n",
    "#Iterate through all powers and assimilate results\n",
    "for i in range(1,16):\n",
    "    coef_matrix_simple.iloc[i-1,0:i+2] = linear_regression(data, power=i, models_to_plot=models_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the display format to be scientific for ease of analysis\n",
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Normalization Ridge Regression\n",
    "- 岭回归是在线性回归的基础上引入L2正则化（Ridge Regression也叫L2正则化）的一种改进方法。\n",
    "- 在目标函数中加入了一个惩罚项，该项是模型权重的平方和乘以一个正则化参数（alpha）。\n",
    "- 这个正则化项有助于防止过拟合，尤其在特征之间存在共线性（高度相关）的情况下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridge_regression(data, predictors, alpha, models_to_plot={}):\n",
    "    \n",
    "    ridgereg = Ridge(alpha=alpha) #,normalize=True)\n",
    "    ridgereg.fit(data[predictors],data['y'])\n",
    "    y_pred = ridgereg.predict(data[predictors])\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([ridgereg.intercept_])\n",
    "    ret.extend(ridgereg.coef_)\n",
    "\n",
    "\n",
    "    #Check if a plot is to be made for the entered alpha\n",
    "    if alpha in models_to_plot:\n",
    "        plt.subplot(models_to_plot[alpha])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred,lw=3)\n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        plt.title('Plot for alpha: %.3g ,RSS : %.2f'%(alpha,rss))\n",
    "    return ret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize predictors to be set of 15 powers of x\n",
    "predictors=['x']\n",
    "predictors.extend(['x_%d'%i for i in range(2,16)])\n",
    "\n",
    "#Set the different values of alpha to be tested\n",
    "alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
    "\n",
    "#Initialize the dataframe for storing coefficients.\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['alpha_%.2g'%alpha_ridge[i] for i in range(0,10)]\n",
    "coef_matrix_ridge = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n",
    "for i in range(10):\n",
    "    coef_matrix_ridge.iloc[i,] = ridge_regression(data, predictors, alpha_ridge[i], models_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有多少个系数为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix_ridge.apply(lambda x: sum(x.values==0),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regulariztion Lasso Regression\n",
    "- Lasso回归（Least Absolute Shrinkage and Selection Operator Regression）也是在线性回归的基础上引入正则化的方法，但是使用的是L1正则化。\n",
    "- L1正则化的惩罚项是模型权重的绝对值之和乘以一个正则化参数（alpha）。\n",
    "- Lasso回归有助于产生稀疏权重，即使对于大量特征，只有一部分特征的权重会非零，这可以看作是一种特征选择的方法。\n",
    "- L1正则化项是模型参数的绝对值之和乘以一个正则化参数。其数学形式为：\n",
    "- 这个正则化项促使模型中的某些参数趋向于零，因此具有稀疏性，可以用于特征选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def lasso_regression(data, predictors, alpha, models_to_plot={}):\n",
    "    #Fit the model\n",
    "#     lassoreg = Lasso(alpha=alpha,normalize=True, max_iter=1e5)\n",
    "    lassoreg = Lasso(alpha=alpha, max_iter=100000)\n",
    "    lassoreg.fit(data[predictors],data['y'])\n",
    "    y_pred = lassoreg.predict(data[predictors])\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([lassoreg.intercept_])\n",
    "    ret.extend(lassoreg.coef_)\n",
    "    \n",
    "    #Check if a plot is to be made for the entered alpha\n",
    "    if alpha in models_to_plot:\n",
    "        plt.subplot(models_to_plot[alpha])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred,lw=3)\n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        plt.title('Plot for alpha: %.3g'%alpha)\n",
    "    \n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize predictors to all 15 powers of x\n",
    "predictors=['x']\n",
    "predictors.extend(['x_%d'%i for i in range(2,16)])\n",
    "\n",
    "#Define the alpha values to test\n",
    "alpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 1, 5, 10]\n",
    "\n",
    "#Initialize the dataframe to store coefficients\n",
    "col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]\n",
    "ind = ['alpha_%.2g'%alpha_lasso[i] for i in range(0,10)]\n",
    "coef_matrix_lasso = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "#Define the models to plot\n",
    "models_to_plot = {1e-10:231, 1e-5:232,1e-4:233, 1e-3:234, 1e-2:235, 1:236}\n",
    "\n",
    "#Iterate over the 10 alpha values:\n",
    "for i in range(10):\n",
    "    coef_matrix_lasso.iloc[i,] = lasso_regression(data, predictors, alpha_lasso[i], models_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_matrix_lasso.apply(lambda x: sum(x.values==0),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则化项是在机器学习中用于控制模型复杂度的一种技术。\n",
    "\n",
    "它在模型的损失函数中引入一个额外的项，这个项的目的是限制模型参数的大小，从而防止过拟合。\n",
    "\n",
    "在线性回归中，损失函数通常是最小化实际观测值与模型预测值之间的差异。正则化项添加到损失函数中，可以采用L1正则化（Lasso）或L2正则化（Ridge）。\n",
    "\n",
    "L1正则化（Lasso）：\n",
    "\n",
    "L1正则化项是模型参数的绝对值之和乘以一个正则化参数。其数学形式为：$$\\lambda \\sum_{i=1}^{n} |w_i|$$\n",
    "这个正则化项促使模型中的某些参数趋向于零，因此具有稀疏性，可以用于特征选择。\n",
    "L2正则化（Ridge）：\n",
    "\n",
    "L2正则化项是模型参数的平方和乘以一个正则化参数。其数学形式为：$$\\lambda \\sum_{i=1}^{n} w_i^2 $$\n",
    "这个正则化项对参数进行平滑，防止参数过大，有助于处理共线性问题。\n",
    "正则化项的引入可以控制模型的复杂度，防止模型在训练数据上过拟合。正则化参数 \\lambdaλ 是一个调整项，用于平衡模型的拟合和正则化项的影响，其值越大，正则化的影响越强。选择合适的正则化参数对于获得泛化性能好的模型非常重要。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
